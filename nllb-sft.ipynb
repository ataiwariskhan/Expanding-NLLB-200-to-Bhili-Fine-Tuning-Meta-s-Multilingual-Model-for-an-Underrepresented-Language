{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets sentencepiece accelerate evaluate sacrebleu rouge-score\n\nimport os\nimport json\nimport gc\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\nimport torch\nfrom torch import nn\n\nimport sentencepiece as spm\nfrom sentencepiece import sentencepiece_model_pb2 as sp_proto\n\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    NllbTokenizer,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T06:58:08.379596Z","iopub.execute_input":"2025-09-20T06:58:08.380030Z","iopub.status.idle":"2025-09-20T06:59:54.229995Z","shell.execute_reply.started":"2025-09-20T06:58:08.379987Z","shell.execute_reply":"2025-09-20T06:59:54.229087Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-09-20 06:59:36.449620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758351576.634558      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758351576.687320      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"BASE_MODEL = \"facebook/nllb-200-distilled-600M\"\nWORKDIR = \"/kaggle/working/\"\nos.makedirs(WORKDIR, exist_ok=True)\n\nCSV_PATH = \"/kaggle/input/supervise-nllb/top_3000_rows.csv\"  # expects columns: Hindi, Bhili\nSPM_PREFIX = os.path.join(WORKDIR, \"spm_bhili\")\nMERGED_SPM_PATH = os.path.join(WORKDIR, \"sentencepiece.bpe.model\")\n\nSRC_LANG = \"hin_Deva\"   # Hindi (existing NLLB tag)\nNEW_LANG = \"bhb_Deva\"   # Bhili in Devanagari; use \"bhb_Gujr\" if Gujarati script\nMAX_LENGTH = 256\nBATCH_SIZE = 2\nNUM_EPOCHS = 3\nLR = 5e-5\nOUTPUT_DIR = os.path.join(WORKDIR, \"finetuned-hin-bhb\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:03:29.298740Z","iopub.execute_input":"2025-09-20T07:03:29.299116Z","iopub.status.idle":"2025-09-20T07:03:29.305691Z","shell.execute_reply.started":"2025-09-20T07:03:29.299086Z","shell.execute_reply":"2025-09-20T07:03:29.304940Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(CSV_PATH)\ndf = df.rename(columns={c: c.strip() for c in df.columns})\ndf = df.dropna(subset=[\"Hindi\", \"Bhili\"])\ndf = df[[\"Hindi\", \"Bhili\"]].reset_index(drop=True)\nlen(df), df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:03:34.291569Z","iopub.execute_input":"2025-09-20T07:03:34.291892Z","iopub.status.idle":"2025-09-20T07:03:34.354262Z","shell.execute_reply.started":"2025-09-20T07:03:34.291867Z","shell.execute_reply":"2025-09-20T07:03:34.353475Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(999,\n                                                Hindi  \\\n 0  वे महाराष्ट्र के चंद्रपुर संसदीय क्षेत्र से 16...   \n 1  उन्होंने 1972 बैच के आंध्रप्रदेश कैडर के भारती...   \n 2  वे आंध्रप्रदेश कैडर के 1972 बैच के भारतीय प्रश...   \n 3  उन्होंने 1975 के बैच के भारतीय रेल सेवा के सिग...   \n 4  1975 के बैच के सिग्‍नल अभियंताओं के भारतीय रेल...   \n \n                                                Bhili  \n 0  त्यां महारास्ट्र नां सन्द्रपुर संसदीय ईलाका थी...  \n 1  तिनायीं 1972 पाळी ना आंध्रप्रदेस कैडर ना भारती...  \n 2  त्यां आंध्रप्रदेस कैडर ना 1972 पाळी नां भारतीय...  \n 3  तिहुयें 1975 नी पाळी ना भारतीय रेल सेवा ना सिग...  \n 4  1975 नां बैस ना सिग्नल इनजींनीयर नीं भारतीय रे...  )"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"#  Building a Custom Tokenizer","metadata":{}},{"cell_type":"code","source":"bhili_corpus_path = os.path.join(WORKDIR, \"bhili_corpus.txt\")\ndf[\"Bhili\"].to_csv(bhili_corpus_path, index=False, header=False)\n\nspm.SentencePieceTrainer.train(\n    input=bhili_corpus_path,\n    model_prefix=SPM_PREFIX,\n    vocab_size=2100,\n    character_coverage=1.0,\n    model_type=\"unigram\",\n    shuffle_input_sentence=True,\n    num_threads=4,\n    bos_id=0,\n    eos_id=2,\n    pad_id=1,\n    unk_id=3,\n    add_dummy_prefix=True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:03:40.079212Z","iopub.execute_input":"2025-09-20T07:03:40.079959Z","iopub.status.idle":"2025-09-20T07:03:40.187234Z","shell.execute_reply.started":"2025-09-20T07:03:40.079929Z","shell.execute_reply":"2025-09-20T07:03:40.186427Z"}},"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/bhili_corpus.txt\n  input_format: \n  model_prefix: /kaggle/working/spm_bhili\n  model_type: UNIGRAM\n  vocab_size: 2100\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 4\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 3\n  bos_id: 0\n  eos_id: 2\n  pad_id: 1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/working/bhili_corpus.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 999 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=85581\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=106\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 999 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=55493\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 4686 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 999\ntrainer_interface.cc(609) LOG(INFO) Done! 2984\nunigram_model_trainer.cc(602) LOG(INFO) Using 2984 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2453 obj=11.2955 num_tokens=6277 num_tokens/piece=2.55891\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2118 obj=9.973 num_tokens=6319 num_tokens/piece=2.98347\ntrainer_interface.cc(687) LOG(INFO) Saving model: /kaggle/working/spm_bhili.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: /kaggle/working/spm_bhili.vocab\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Training SentencePiece and Adding New-Tag","metadata":{}},{"cell_type":"code","source":"tok_old = NllbTokenizer.from_pretrained(BASE_MODEL)\nsp_old = sp_proto.ModelProto()\nsp_old.ParseFromString(tok_old.sp_model.serialized_model_proto())\n\nsp_trained = spm.SentencePieceProcessor(model_file=f\"{SPM_PREFIX}.model\")\nsp_new = sp_proto.ModelProto()\nsp_new.ParseFromString(sp_trained.serialized_model_proto())\n\nexisting = {p.piece for p in sp_old.pieces}\nmin_score = min(p.score for p in sp_old.pieces)\n\nadded = 0\nfor p in sp_new.pieces:\n    if p.type != 1:  # normal pieces only\n        continue\n    if p.piece not in existing:\n        new_p = sp_proto.ModelProto().SentencePiece()\n        new_p.piece = p.piece\n        new_p.score = min_score - 1.0\n        sp_old.pieces.append(new_p)\n        added += 1\n\nwith open(MERGED_SPM_PATH, \"wb\") as f:\n    f.write(sp_old.SerializeToString())\n\nprint(f\"Added {added} new pieces into merged SentencePiece model at {MERGED_SPM_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:03:46.282589Z","iopub.execute_input":"2025-09-20T07:03:46.283191Z","iopub.status.idle":"2025-09-20T07:03:56.286231Z","shell.execute_reply.started":"2025-09-20T07:03:46.283165Z","shell.execute_reply":"2025-09-20T07:03:56.285327Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"926edf0cded24ac5b6d71f3a21f79595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e5624ec5c4d4459baccb68e563a3f67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a80caa6a43e34ce395f5228fa2907fd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e54cd2f385549fbb15fc4828c3e97f9"}},"metadata":{}},{"name":"stdout","text":"Added 941 new pieces into merged SentencePiece model at /kaggle/working/sentencepiece.bpe.model\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Merging Tokenizer","metadata":{}},{"cell_type":"code","source":"tok_merged = NllbTokenizer.from_pretrained(\n    BASE_MODEL,\n    vocab_file=MERGED_SPM_PATH,\n)\n\nadditional = list(tok_merged.additional_special_tokens)\nif NEW_LANG not in additional:\n    additional.append(NEW_LANG)\ntok_merged.add_special_tokens({\"additional_special_tokens\": additional})\n\nprint(\"Tokenizer size after SP merge:\", len(tok_merged))\nprint(\"New language token id:\", tok_merged.convert_tokens_to_ids(NEW_LANG))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:04:05.047198Z","iopub.execute_input":"2025-09-20T07:04:05.047999Z","iopub.status.idle":"2025-09-20T07:04:07.681728Z","shell.execute_reply.started":"2025-09-20T07:04:05.047967Z","shell.execute_reply":"2025-09-20T07:04:07.680896Z"}},"outputs":[{"name":"stdout","text":"Tokenizer size after SP merge: 256943\nNew language token id: 256942\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\nmodel.resize_token_embeddings(len(tok_merged))\n\nwith torch.no_grad():\n    new_id = tok_merged.convert_tokens_to_ids(NEW_LANG)\n    hin_id = tok_merged.convert_tokens_to_ids(SRC_LANG)\n    if new_id is not None and hin_id is not None:\n        model.model.shared.weight[new_id] = model.model.shared.weight[hin_id].clone()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:04:13.948922Z","iopub.execute_input":"2025-09-20T07:04:13.949201Z","iopub.status.idle":"2025-09-20T07:04:47.803747Z","shell.execute_reply.started":"2025-09-20T07:04:13.949181Z","shell.execute_reply":"2025-09-20T07:04:47.802932Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2aad72ef95a4e429b09ed045a0229c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74f315f1b450446f891c962a5bb202e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc8282ba4ee44d2fb0b3eccca38b7a0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd334ebffb0c4f098252030ca8be3db1"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"tok_merged.src_lang = SRC_LANG\ntok_merged.tgt_lang = NEW_LANG\n\ndef preprocess(batch):\n    return tok_merged(\n        batch[\"Hindi\"],\n        text_target=batch[\"Bhili\"],\n        max_length=MAX_LENGTH,\n        truncation=True,\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:04:52.971905Z","iopub.execute_input":"2025-09-20T07:04:52.972416Z","iopub.status.idle":"2025-09-20T07:04:52.980032Z","shell.execute_reply.started":"2025-09-20T07:04:52.972375Z","shell.execute_reply":"2025-09-20T07:04:52.979137Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"split_idx = int(0.9 * len(df))\ntrain_df = df.iloc[:split_idx].copy()\nval_df = df.iloc[split_idx:].copy()\n\nds = DatasetDict({\n    \"train\": Dataset.from_pandas(train_df, preserve_index=False),\n    \"validation\": Dataset.from_pandas(val_df, preserve_index=False),\n})\n\ntokenized = ds.map(preprocess, batched=True, remove_columns=[\"Hindi\", \"Bhili\"])\ntokenized\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:04:58.952150Z","iopub.execute_input":"2025-09-20T07:04:58.952431Z","iopub.status.idle":"2025-09-20T07:04:59.475705Z","shell.execute_reply.started":"2025-09-20T07:04:58.952410Z","shell.execute_reply":"2025-09-20T07:04:59.475008Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/899 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfbbcc2890bd44218cf4e53616a20b3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2c3c32d3c6439285f1e990f5b366fa"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 899\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 100\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"collator = DataCollatorForSeq2Seq(tokenizer=tok_merged, model=model, padding=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:05:03.601025Z","iopub.execute_input":"2025-09-20T07:05:03.601312Z","iopub.status.idle":"2025-09-20T07:05:03.605602Z","shell.execute_reply.started":"2025-09-20T07:05:03.601291Z","shell.execute_reply":"2025-09-20T07:05:03.604428Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    learning_rate=LR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    logging_steps=50,\n    predict_with_generate=True,\n    fp16=torch.cuda.is_available(),\n    report_to=[],\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    data_collator=collator,\n    tokenizer=tok_merged,\n)\n\ntrainer.train()\ntrainer.save_model(OUTPUT_DIR)\ntok_merged.save_pretrained(OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:05:10.501537Z","iopub.execute_input":"2025-09-20T07:05:10.501835Z","iopub.status.idle":"2025-09-20T07:11:17.492096Z","shell.execute_reply.started":"2025-09-20T07:05:10.501813Z","shell.execute_reply":"2025-09-20T07:11:17.490817Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/160476912.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1351' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1350/1350 05:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.472000</td>\n      <td>3.356400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.864200</td>\n      <td>3.096367</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.734100</td>\n      <td>3.016486</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/38: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/160476912.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtok_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2657\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3335\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3336\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3337\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3339\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 3662915840 vs 3662915728"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:626] . unexpected pos 3662915840 vs 3662915728","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# !pip install -q transformers sacrebleu pandas\n\nimport os, re, torch, pandas as pd\nfrom typing import List\nfrom transformers import AutoModelForSeq2SeqLM, NllbTokenizer\nimport sacrebleu\n\n# --------------------------\n# Paths and language settings\n# --------------------------\n# If uploaded as Kaggle dataset:\n# BASE_DIR = \"/kaggle/input/my-finetuned-nllb\"  # contains checkpoint-XXXX or the saved model files\n# If using current working directory outputs:\n# BASE_DIR = \"/kaggle/working/finetuned-hin-bhb\"\n\nBASE_DIR = \"/kaggle/working/finetuned-hin-bhb/checkpoint-1350\"   # change me\nSRC_LANG = \"hin_Deva\"   # source language tag present in NLLB\nTGT_LANG = \"bhb_Deva\"   # target tag you used during fine-tuning (e.g., Bhili in Devanagari)\n\n# Evaluation CSV with columns: source and reference\nEVAL_CSV = \"/kaggle/input/supervise-nllb2/1k_test.csv\"  # change me\nSRC_COL = \"Hindi\"      # source column name\nREF_COL = \"Bhili\"      # reference column name\n\nMAX_LENGTH = 256\nBATCH_SIZE = 4\nNUM_BEAMS = 4\n\n# --------------------------\n# Pick checkpoint directory\n# --------------------------\nckpts = [d for d in os.listdir(BASE_DIR) if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(BASE_DIR, d))]\nif not ckpts:\n    CHECKPOINT_DIR = BASE_DIR  # model saved directly at root\nelse:\n    steps = [(int(re.findall(r\"checkpoint-(\\d+)\", d)[0]), d) for d in ckpts if re.findall(r\"checkpoint-(\\d+)\", d)]\n    steps.sort()\n    CHECKPOINT_DIR = os.path.join(BASE_DIR, steps[-1][1])\nprint(\"Using checkpoint:\", CHECKPOINT_DIR)\n\n# --------------------------\n# Load model and tokenizer\n# --------------------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT_DIR).to(device)\ntokenizer = NllbTokenizer.from_pretrained(CHECKPOINT_DIR, src_lang=SRC_LANG, tgt_lang=TGT_LANG)\nforced_bos_id = tokenizer.convert_tokens_to_ids(TGT_LANG)\nassert forced_bos_id is not None, f\"Target language token {TGT_LANG} not found in tokenizer.\"\n\n# --------------------------\n# Generation function\n# --------------------------\n@torch.no_grad()\ndef translate(texts: List[str]) -> List[str]:\n    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH).to(device)\n    gen = model.generate(\n        **enc,\n        forced_bos_token_id=forced_bos_id,  # critical for NLLB target control\n        max_length=MAX_LENGTH,\n        num_beams=NUM_BEAMS\n    )\n    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n# --------------------------\n# Load eval data and translate\n# --------------------------\ndf = pd.read_csv(EVAL_CSV)\nsources = df[SRC_COL].fillna(\"\").astype(str).tolist()\nreferences = df[REF_COL].fillna(\"\").astype(str).tolist()\n\npredictions = []\nfor i in range(0, len(sources), BATCH_SIZE):\n    batch = sources[i:i+BATCH_SIZE]\n    predictions.extend(translate(batch))\n\n# --------------------------\n# Scoring: BLEU and chrF2\n# --------------------------\n# Feed detokenized strings to SacreBLEU and let it handle tokenization for BLEU.\n# chrF2 is the default for chrF in SacreBLEU (beta=2).\nbleu = sacrebleu.corpus_bleu(predictions, [references])         # BLEU\nchrf2 = sacrebleu.CHRF(word_order=0, beta=2).corpus_score(       # chrF2 (character n-grams only)\n    predictions, [references]\n)\n\nprint(f\"BLEU = {bleu.score:.2f}\")\nprint(f\"chrF2 = {chrf2.score:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:54:45.836443Z","iopub.execute_input":"2025-09-20T07:54:45.836943Z","iopub.status.idle":"2025-09-20T07:57:56.286457Z","shell.execute_reply.started":"2025-09-20T07:54:45.836920Z","shell.execute_reply":"2025-09-20T07:57:56.285697Z"}},"outputs":[{"name":"stdout","text":"Using checkpoint: /kaggle/working/finetuned-hin-bhb/checkpoint-1350\nBLEU = 8.41\nchrF2 = 34.60\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}